 ---
output:
  html_document: default
  pdf_document: default
  word_document: default
---

# *UNIX/BASH COMMANDS*
## Basic commands refresher
### Server runs
To log into the server `ssh ____@agri.cse.ucdavis.edu`   where ___ is your username

To check what is running on the server right now `top`

To start a program on the server and not the head node `sbatch ____`
  - Must use -t for time needed to run program so `sbatch -t 24:00:00 __` would be for 24 hrs
  - Must use -p for imporance (low, med, high) so `sbatch -t time -p high ___` for some time with high importance
**Although not always shown below, all sbatch must have -t and -p or they will fail to run**
  - Added memory is also posssible `sbatch --mem=16G -t time - p high ___` For memory intensive jobs (16, 32, 64, etc.)

To check whether sbatch is running, type `smap -c | grep your_user_name`

To check error files `less slurm ___` The latest file should have the highest number

To cancel server tasks `scancel [JOBNUMBER]`

### Word count/file counts
 - `wc -l` the fastq files
 - `ls -l *.hash | wc -l` count files in a dir

### Check File Sizes
 - `du -hs * | sort -hs`  (use "*RUBIC-PH*")

### Screens
 - Screen is a full-screen window manager that multiplexes a physical terminal between several processes, typically shells.
 - A way to run mutliple windows (usually programs) at one time
 - Controlled with  (Ctrl a + one other keystroke)
 - New Screen `screen`
 - List screens `screen -ls`
 - Command keys (`Ctrl a + ?`) or Key Binding
 - Screen detach (`Ctrl a + d`)
 - to kill screen `screen -r [name]` then `exit`

### Text Editors
While there are many available, we prefer to use VIM

Find and replace many/every line

 - :%s: everyline
 - :%s:AMER:../AMER:
 - Alternatively you can say :7,12: for lines 7 through 12


### Bioinformatic setup
There are two ways of approaching all the scripts and programs necessary. Keep them in your /bin ($PATH), in a separate directory (like /scripts) or copy and paste them in the directory you are working in. The last way allows for project specific changes without changing the original script. 

---

### General Genomic Information
#### Cutters
 - Six cutter   (5' -  CTGCA_G - 3')    Pst1
 - Eight cutter (5' - CCTGCA_GG - 3')   Sbf1

 - Biotin-PlateBarcode-Sbf1-CC-IndivBarcode-CCTGCA_GG_Fragment

#### Q Phred Quality scores
Initial files are .fastq because they contain Q quality scores

- there are 4 lines per each individuals sequence
 - score is series of characters->numbers->CAPletters->lowercaseletters      lowest->highest
- Q represents the quality score of nucleotides generated by automated DNA seq
   -  Q = -10log10 P where P = base calling error probability
   - 50 would be 99.999%; 20 would be 99%; 10 would be 90%
   - Most common is +33

---

# *DOWNLOADING ORIGINAL ILLUMINA FILES*
 - All files come as zipped .fastq files
 
1. Make a new directory `mkdir SOMM_`

2. Files are always downloaded here
   `cp ~millermr/UCDavis/SOMM_/*index_* ./`
 - to learn plate barcode, open info.txt in SOMM folder SOMM____ INDEX___
 
3. `gunzip s*`

4. To keep things clean, remove the zip file (not the unzipped ones though)

5. Obtain the following scripts
  - BarcodeSplitListBestRadPairedEnd.pl 
  - run_BestRadSplit_.sh (underscore is either a 6 or an 8 - depending on enzyme used)
  
6. If you have multiple runs, you should concatenate them together
 - Example `cat S*_R1.* > SOMM?_index?_ATTCCT_L001_R1.fastq`
 - Example `cat S*_R2.* > SOMM?_index?_ATTCCT_L001_R2.fastq`
 
7. Use cat'd files (if necessary) to run run_BestRadSplit_.sh
 - This script runs the perl script BarcodeSplitListBestRadPairedEnd.pl and looks at R1 and R2 to find which has the Indiv. barcode (because the probe sits down randomly on either), then puts all the fragments with barcodes as RA and those without as RB. This should give you the same # of lines.
 - Script bases matches on location in file (as sorted) but a numeric value is available
 - RA will be slightly smaller (use `du -hs`) because the barcode is cleaved (but still in the name of the file)
 
8. Obtain your metadate file. This file contains the name of each individual, its location on the plate, etc.

 <span style="color:red"> This part can be done now or later. But be absolutely sure that your indiv names, their plate location (A1, B1, etc.), and the appropriate barcode are all in sync. For example, the barcodes might be A1 A2 A3 (Across the plate row) while your sample names are A1 B1 C1 (Down the plate column)</span> 

9. Replace barcode with name of Indiv using Metadata file and script Barcode_to_Name
 - `sbatch -t 2880 -p high Barcode_to_Name metadatafile`  2880 is the number of minutes (estimate to run - adjustable)
 
10. Concatenate each individuals multiple files into one RA, RB per indiv.
 - `sbatch -t 2880 -p high Concatenate.sh` You might need to hardcode the metadata file into this script.
 
11. Keep concatenated files, remove all others. They should be something like NAME1_RA.fastq and NAME1_RB.fastq

**The de novo step is not necessary if a model genome is available (skip to Align individuals to loci)**   

---

# *DE NOVO ASSEMBLY*
## Choosing Individuals
1. Pick your best 5 - 10 individuals (using du -hs or wc -l)

2. Copy these individuals (just RA) into a new directory

## Option 1 (Clean up reads in a single script)
1. Make an indiv file list (for however many files you chose). Start by making a list of files using sed:
    - `ls fastq | sed "s:.fastq::g" > list`
    
2. Modify and run the script Hash_reads.sh, which will run all three perl scripts (see Option 2)

## Option 2 (Clean up reads one script/Indiv at a time)
1. Modify file for quality control by throwing out seqs which do not meet threshold
 - Example `perl QualityFilter.pl Dpup_007_RA.fastq > Dpupt_oo7_RA_QF.fastq`
 - Do for all individuals you chose
 
2. Find and shrink seqs to only unique ones while culling down file from 4 lines to 2 lines
 - `perl HashSeqs.pl Dpup_007_RA_QF.fastq Dpup_007 > Dpup_007_RA_QF.hash`
 - Do for all indivduals you chose
 
3. Depending on coverage, you can look at the distribution and see the # of occurrences vs. # of sequences, expect a peak a some point followed by a dip and a recovery, with a spike early on (usually errors).
  - `perl PrintHashHisto.pl Dpup_007_RA_QF.hash`
  - Do for all individuals you chose
  
4. May need more samples because don't have many sequences within the distribution (low coverage)

## Alignment
Before aligning, need to know parameters for alignment script. To count files in a dir:
> `ls -l *.hash | wc -l`
Concatenate all the hash files `cat *.hash > _.fasta` where the underscore can be anything you want (like a name)
  - Make sure novoalign (a program) is in your bin or PATH or copied to this directory

### Index
1. Create the novoindex index file (can run with 32G if need more memory)
 - `screen` See notes at start of this pipeline in case you are unfamiliar
 - `srun --mem=16G novoindex _.fa.idx _.fasta` This is what you want it called and whatever your fasta file was named 
 - Use smap to view task `smap -c | grep your_user_name`

### Align
2. Now align index to itself, with following:
  - `screen` See notes at start of this pipeline in case you are unfamiliar
  - `srun --mem=16G novoalign -r E 48 -t 180 -d _.fa.idx -f _.fasta > _.novo`
  - Output is a **"_.novo"_** file
  - Check size of file with `du -hs * | sort -hr`

## ID loci
1. Now identify loci: 
 - `screen` See notes at start of this pipeline in case you are unfamiliar
 - `srun --mem=16G IdentifyLoci3.pl _.novo > _IDloci.fasta`

2. Once done counting loci, divide by 2 to get idea of how many loci we are dealing with, perl can only handle 8000 lines and 4000 loci chunks (*We will need to split _IDloci.fasta into 8000 line chunks*)
 - See how many polymorphic loci exist
`perl SimplifyLoci2.pl _IDloci.fasta | grep "_2" | wc -l`

## Price (Extension)

1. Now create PRICE directory and extendLoci directories inside PRICE
 - `mkdir PRICE [in denovo folder]`
 - `mkdir extendLoci_1` (if 104,000 or less (aa:az)) #4000 loci x 26 letters (a-z) equals 104,000
 - `mkdir extendLoci_2` (if >104,000 loci)           #add another directory for each 104000 you will need. 
    - This is common when a six cutter is used because more loci will be generated.

2. Make sure you have scripts required in the PRICE directory:
 - `cp ~PATH/RecoverLocusSpecificReads.pl ./`  This must be in the PRICE directory
   - `getLoci.py`                              
   - `RecoverLocusSpecificReads.sh`
   - `extendLoci.sh`
   - `format_contigs.sh`
   - `cat.sh`
   - `select_loci.py`                          This must be in the PRICE directory

3. Cat all your R1 & R2 files into one file each (one R1 and one R2) and copy to PRICE:
   *Depending on the size and number of seq, may be best to sbatch*
   - `cat *_RA.fastq > RABO_R1.fastq` For example RABO is the species name and its all individuals sequenced
   - `cat *_RB.fastq > RABO_R2.fastq`
   - `cp RABO* ./PRICE`

4. wc -l the ID Loci file and divide by 2 to see how many loci
 - `????? / 2 =` **?????? LOCI** using `IDLoci2`

5. Strip names off of files to make a simplified version:
 - `perl ~/scripts/SimplifyLoci2.pl _IDloci.fasta | grep --no-group-separator -A 1 "_1" > _IDloci_s.fasta`
 - Then remove the _1 and make a `simplified final` version
   - `sed 's/_1//' _IDloci_s.fasta > _IDloci_sf.fasta`

6. Split files into 8000 line chunks in the PRICE folder
  - `split -l 8000 _IDloci_sf.fasta _IDloci_sf.fasta'_'`  This should give you the same file name plus aa,ab,ac,etc at the end of the file name
  - Double check with tail and wc -l to make sure each chunk is 8000 (except last one)

7. Make a list of all the output files you just made (example _IDloci_sf.fasta_aa, _IDloci_sf.fasta_ab)
 - `ls *.fasta_a* > data_list`
 
8. To get an idea of how many loci you have, open the last file created by splitting and 
`tail _IDloci_sf.fasta_a?` and the last number is the number of loci you have (R??????) OR
you can use the number of loci you found in #4 (good to check both really)

9. In PRICE create a `Loci_aa` file which is a list of R000001 to whatever locus number you have from #8 (up to R104000) and (*if necessary*) a `Loci_bb` which is R104001 to whatever locus number you have. You can use the Rscript (`makelist.R`) or use a text editor
 - Make a list of each line number (i.e., R000001 >> ?), use Rscript `makelist.R`
  `numbs<-seq(1:108279)` Again this is assuming you have 108279 loci
  `rabo<-sprintf("R%06d", numbs)`
  `write.table(rabo, quote=FALSE, col.names = FALSE, row.names = F)`

 - `Rscript makelist.R` Should provide Loci_aa and Loci_bb files

### Adjust Scripts

10. Copy the following scripts into each `extendLoci_*` directory you created
   - `RecoverLocusSpecificReads.sh`
   - `extendLoci.sh`
   - `format_contigs.sh`
   - `cat.sh`
   - `getLoci.py` 
   
11. VIM (or another text editor) `RecoverLocusSpecificReads.sh` to reflect your ../data_list and two fastq files (../_R1.fastq and ../_R2.fastq). Also adjust $x -le ? to reflect the number of files in your data_list (so if you have aa->ag, ? = 7)
 - This builds shell scripts in the PRICE directory for each aa -> whatever that you have

12. VIM `extendLoci.sh` and adjust as needed:
 - Change the number(?) in $x -le ? to whatever your total loci number is from #8 (*max 104000*) which covers the aa up to a? files in `extendLoci_1.sh`

13. VIM `format_contigs.sh` and adjust as needed:
- Change the number(?) in $x -le ? to whatever your total loci number is from #8 (*max 104000*) which covers the aa up to a? files in `format_contigs.sh`

## Run scripts in each extendLoci directory IN ORDER from within that directory:

14. Now run `RecoverLocusSpecificReads.sh` in `extend_Loci_?` directory
 - `sbatch RecoverLocusSpecificReads.sh with Loci list (data_list)`to create sh files in PRICE 

15. Then run these from the extendLoci_* directories, one at a time, in order. 
 - `sbatch extendLoci.sh ../Loci_aa`
 - `sbatch format_contigs.sh ../Loci_aa`  
 - `sbatch cat.sh aa`  

 - Do the same for Loci_bb (which would be R104000->whatever) in extendLoci_2 **if necessary**
 
16. To obtain the file you need (aa_contigs.fasta) which was output from cat.sh, and needs to be in PRICE, `mv aa_contigs.fasta ../`

<span style="color:red">Be careful not to "ls" the extendLoci_* directories as they are very full and may stall your command line</span> 

17. Cat Loci_aa and Loci_bb together, if necessary
`cat Loci_??_contigs.fasta > final_contigs.fasta`

18. Then select loci with 300 bp or more
`python select_loci.py final_contigs.fasta 300`
   - word count to see total loci `wc -l final_contigs`
   - The number of total loci is **??????/2** #4 or #8 above
   - The number of total loci with minimum length 300 is **??????/2** #18

---
 
# *ALIGN INDIVIDUALS TO LOCI*

 1. Build index of contigs: `bwa index final_contigs_300.fasta` OR `A_model_genome.fasta`
  - Files will now end in .amb, .ann, etc.
 2. Using all `fastq` build a list of the fastq R1 and R2 files into different columns
   - `ls *_??_?_R2* > listR2` and `ls *_??_?_R1* > listR1`
   - Then paste lists together into columns: `paste listR1 listR2 | sed 's/\.fastq//g' > list`
 3. Obtain the run_align.sh script  For example copy `cp ~millermr/common/run_align.sh ./`
   - `sh run_align.sh list ~/projects/rabo/PRICE/final_contigs_300.fasta`
   - It's taking all fastq files (R1 and R2) and aligning to the _300.fasta file (or model genome file)
4. This should create your final bam files   

# *SUBSAMPLING WITH SAMTOOLS*

- Look at Individual Alignments
 - `samtools flagstat ALAME_AH_2_R1.sort.flt.bam`   Could also be done on _.sort.bam
 
Example: 
> 94696 + 0 in total (QC-passed reads + QC-failed reads)
0 + 0 duplicates
94696 + 0 mapped (100.00%:-nan%)
94696 + 0 paired in sequencing
47492 + 0 read1
47204 + 0 read2
94696 + 0 properly paired (100.00%:-nan%)
94696 + 0 with itself and mate mapped
0 + 0 singletons (0.00%:-nan%)
1458 + 0 with mate mapped to a different chr
655 + 0 with mate mapped to a different chr (mapQ>=5)

- Look at sizes of alignments
  - `du -hs *.sort.flt.bam | sort -h`
  - Note that these are sorted filtered bam files
- Need to pick a threshold for sizes, we generally like alignments over 100,000.

Example:
> 332K	*NFAME_IH_3_R1.sort.flt.bam*
  468K	*NFAME_IH_2_R1.sort.flt.bam*
8.0M	ALAME_AH_2_R1.sort.flt.bam
25M	NFMFA_SC_3_R1.sort.flt.bam
31M	NFAME_IH_1_R1.sort.flt.bam
33M	MFAME_GC_1_R1.sort.flt.bam
34M	NFAME_RR_1_R1.sort.flt.bam
34M	RUBIC_PH_1_R1.sort.flt.bam
35M	ALAME_AC_1_R1.sort.flt.bam
36M	ALAME_AC_3_R1.sort.flt.bam
36M	ALAME_AH_1_R1.sort.flt.bam
37M	RUBIC_PH_2_R1.sort.flt.bam
38M	ALAME_AC_2_R1.sort.flt.bam
38M	ALAME_AH_3_R1.sort.flt.bam
38M	MFAME_GC_2_R1.sort.flt.bam
38M	NFMFA_SC_2_R1.sort.flt.bam
39M	NFAME_RR_2_R1.sort.flt.bam
40M	NFAME_RR_3_R1.sort.flt.bam
41M	MFAME_TC_1_R1.sort.flt.bam
41M	MFAME_TC_2_R1.sort.flt.bam
41M	NFMFA_SC_1_R1.sort.flt.bam
44M	MFAME_SG_1_R1.sort.flt.bam
44M	MFAME_SG_2_R1.sort.flt.bam
44M	RUBIC_PH_3_R1.sort.flt.bam

**Copy script**
 - `cp ~jbaumste/Roach_Hitch/Roach/sub_sample.sh ./`   Could be obtained from anywhere; this is just an example

### Filter out files that are not sufficient
**Make a bam sublist**
- Need to make a bamlist of samples that only have the proper number of alignments (select a threshold of preference, typically 120k for salmon).
**Check each individual to find where alignments get too low - remove those individuals**
- To check where alignments get too low, run a quick pca with different subsamples (e.g. 90k, 80k, etc. till structure falls apart) 

- Make a list of the bam files (and remove bam)
 - `ls *flt.bam | sed 's/\.bam//g' > sublist` 

**Subset to a certain number**
 - `sbatch sub_sample.sh sublist 100000`

**Create a new subsampled bamlist**
- `ls *_100000.bam > _subbamlist`  

# *PCA*
**This is a general way to generate a principal component analysis from your subsampled bam files**

1. Obtain scripts:   pca_calc.sh and plotPCA_2.R and pca_plot.sh

2. Run pca_calc.sh
- `sbatch pca_calc.sh subbamlist out`        (out can be any name for your outputted file)

3. `sed 's/_RA\.sort\.flt_100000\.bam/ 1 1/g' subbamlist > out.clst`

4. VIM (or other text editor) out.clst and insert a header FID IID CLUSTER   (IID changes symbol; CLUSTER changes color)
  - Feel free to modify this file according to symbol or color as you see fit for the final PCA plot
  
5. `sh pca_plot.sh`  This should create 3 pdfs (but can be modified by modifying the script)

## Connect to Cluster to put these files in your local directory (your computer)

- Open a new shell window using `ctrl t`

> `sftp __@agri.cse.ucdavis.edu` where __ is your username for the server
> `cd _/_`  navigate to whichever directory has these files
> `get *pdf` Or download each file independently

- Incidentially to put something on the server

Make sure you are in the local directory that contains the file(s)

> `put ___`  puts it in your home directory on the server

# *ANGSD*
**This is the primary program for analyzing the data from a population genetics standpoint**
**Be wary of what version you have (on the server or in your local) as scripts may/may not work with different versions** 

This program has too many options to put here but a simple explanation may eventually be done
- For a full understanding, see http://www.popgen.dk/angsd/index.php/ANGSD

# *ADMIXTURE*
1. Obtain the scripts Beagleinput.sh & NGSAdmixture.sh & Multi_K_NGSadmixture.sh & Admix_plot.R

2. Using your bamlist, generate a Beagle input file `sbatch Beagleinput.sh bamlist out`  (out can be any name you want)
3a. To make 1 version of multiple Ks
  - `sbatch NGSAdmixture.sh ___.beagle.gz out K`     (where K can be adjusted in the script) OR
3b. To make multiple versions of multiple Ks
  - `sh Multi_K_NGSadmix.sh __  K`   This currently makes 3 version per K but can be modified in the script
  
4. The results should be various outputs out__.qopt where __ is a number (e.g. 11, 12,13, 21, 22, 23 and so on)

5. You can run an R script `Rscript Admix_plot.R __.qopt bamlist.input K out.pdf`  which will plot the admixture graph

6. For Evanno `grep -h "^best" --no-group-separator NAME.log | cut -c11 - 24 > FILE`

7. An easier method may be to put all ___.qopt files on your local computer and use the simple online gui to make all of your admixture graphs  http://pophelper.com/

# *FST*
1. Obtain scripts: SAF.sh & pwfst.sh

2. Create a bamlist with individuals from only the "population" you want to test

3. Generate a simple allele frequencies file (.saf) for every "population" bamlist you want to compare
  `sbatch -t 24:00:00 - p high SAF.sh bamlist outfile_name ancestral_file` (ancestral file can be __300.fasta)
  
4. For pairwise comparisons between "populations", use two ____.saf.index files you just generated
  - `sbatch -t 24:00:00 -p high pwfst.sh pop1 pop2`  where pop1 is the 1st ___.saf.index and pop2 is the 2nd











